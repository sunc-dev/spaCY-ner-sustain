{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.4"
    },
    "colab": {
      "name": "ner-train-note.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p44YsJwWkP64"
      },
      "source": [
        "# <font size=\"10\">Custom entity recognition </font>\n",
        "## Training data environment setup\n",
        "\n",
        "## Introduction:\n",
        "\n",
        "This notebook is test code to setup new entities and programmtically creating training data to train the NER model. The goal of the project is to tag environmental/sustainability technologies contained with in the systems data stored with in our capital planning systems.\n",
        "\n",
        "To skip to creation of training data and go directly to [model training and model implementation](./ner-model-note.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8Hqg0kIkP65"
      },
      "source": [
        "## SETUP\n",
        "Module downloads (necessary for only first instance): "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgCqcKOikP66",
        "outputId": "fbd20385-9596-4de7-aa08-01c35598bd83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        }
      },
      "source": [
        "!pip install spacy\n",
        "!pip install pdfminer.six\n",
        "!pip install nltk \n",
        "!python -m spacy download en_core_web_md"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (50.3.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (2.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.3.1)\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.6/dist-packages (20201018)\n",
            "Requirement already satisfied: chardet; python_version > \"3.0\" in /usr/local/lib/python3.6/dist-packages (from pdfminer.six) (3.0.4)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.6/dist-packages (from pdfminer.six) (2.2.2)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.6/dist-packages (from pdfminer.six) (3.2.1)\n",
            "Requirement already satisfied: six>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from cryptography->pdfminer.six) (1.15.0)\n",
            "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography->pdfminer.six) (1.14.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography->pdfminer.six) (2.20)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: en_core_web_md==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz#egg=en_core_web_md==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (50.3.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.3.1)\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stCHghCykP6_"
      },
      "source": [
        "Import modules:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5YB0ybIkP7A"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import glob\n",
        "from io import StringIO\n",
        "\n",
        "#pdf miner \n",
        "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
        "from pdfminer.converter import TextConverter\n",
        "from pdfminer.layout import LAParams\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "from pdfminer.high_level import extract_text\n",
        "\n",
        "#SpaCy \n",
        "import en_core_web_md\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from spacy.lang.en import English\n",
        "from spacy.matcher import Matcher\n",
        "from spacy.matcher import PhraseMatcher\n",
        "from spacy.pipeline import Sentencizer\n",
        "from spacy.lemmatizer import Lemmatizer, ADJ, NOUN, VERB\n",
        "from spacy.lookups import Lookups\n",
        "#nltk\n",
        "from nltk import tokenize\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QGmLCd8kSUM",
        "outputId": "8247c36f-b439-426b-c304-168c011b408c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwd-892QkP7Q"
      },
      "source": [
        "## Train data setup\n",
        "The code section below setup the training data for the NER.\n",
        "The is defined in the following steps:\n",
        "* Convert documents into text\n",
        "* Setting up the SpaCy nlp object\n",
        "* Load, parsing and tokenization of text into a list of sentences\n",
        "* Utility function order to parse the tokenized sentences in order to inherit the form:  [(Sentence, {entities: [(start, end, label)]}, ...]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPjmQUu2kP7Q"
      },
      "source": [
        "SpaCy object delcaration and relative path setup:\n",
        "\n",
        "SpaCy requires training data to be in the format: [(Sentence, {entities: [(start, end, label)]}, ...].\n",
        "We first need to create a list of sentences throughout a doc object containing the new entity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rh63pQPCkP7R",
        "outputId": "dacbca32-0d7b-4c5c-d905-ac0c7d5fba13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "#Declare english vocab for our nlp object to define english language sentence boundaries\n",
        "nlp = spacy.blank(\"en\")\n",
        "nlp = English()\n",
        "nlp = en_core_web_md.load()\n",
        "nlp.max_length = 6130000 # or higher\n",
        "\n",
        "nlp.pipeline"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('tagger', <spacy.pipeline.pipes.Tagger at 0x7f1545204cf8>),\n",
              " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x7f1544fd0a08>),\n",
              " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x7f1544fd0a68>)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "add7HM_BkP7V"
      },
      "source": [
        "Declare rules for sentence boundary detection logic and add to the nlp pipe object.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVNMzzh7kP7V",
        "outputId": "c9a0fb99-ba53-4212-bb7d-ee2d6737a6fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "sentencizer = Sentencizer(punct_chars=[\".\", \"?\", \"!\", \"ã€‚\"])\n",
        "nlp.add_pipe(sentencizer, before=\"tagger\")\n",
        "nlp.pipeline"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f1545099b38>),\n",
              " ('tagger', <spacy.pipeline.pipes.Tagger at 0x7f1545204cf8>),\n",
              " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x7f1544fd0a08>),\n",
              " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x7f1544fd0a68>)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmqUNryrkP7Y"
      },
      "source": [
        "Setup import doc import, write tests for single doc object for now:\n",
        "\n",
        "## Current test case is for the word 'green roof'\n",
        "\n",
        "Document path setup (can change to a higher level directory to capture more/different docs for other terms)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrY9wVB2kP7Y",
        "outputId": "4ddf34a5-17b3-4d90-8b5e-4fa48182b5ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "'''doc_path = Path(r'./doc/stormw') #change this for broader directory to capture\n",
        "pdf_objects = list(doc_path.glob('stormw4.pdf'))  # convert result to a list\n",
        "print(pdf_objects)'''\n",
        "\n",
        "#Setup higher-level directory function to import more docs\n",
        "#dir_path = r'./doc/' #change this for broader directory to capture\n",
        "\n",
        "#For collab\n",
        "dir_path = r'/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/doc'\n",
        "pdf_objects = glob.glob(dir_path + '/**/*.pdf', recursive=True)\n",
        "print(pdf_objects)\n",
        "len(pdf_objects)\n",
        "#test on single pdf object\n",
        "#pdf_object =  os.path.join(doc_path, 'example2.pdf')'''"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/doc/backw/backw1.pdf', '/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/doc/backw/backw2.pdf', '/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/doc/backw/backw3.pdf', '/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/doc/backg/backg2.pdf', '/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/doc/backg/backg1.pdf', '/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/doc/backp/backp1.pdf', '/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/doc/fuels/fuels3.pdf', '/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/doc/fuels/fuels2.pdf', '/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/doc/fuels/fuels1.pdf', '/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/doc/downs/downs1.pdf', '/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/doc/downs/downs4.pdf', '/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/doc/downs/downs3.pdf', '/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/doc/downs/downs2.pdf', '/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/doc/floodb/floodb1.pdf', '/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/doc/floodb/floodb2.pdf', '/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/doc/roofd/roofd2.pdf', '/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/doc/roofd/roofd1.pdf', '/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/doc/rainw/rainw1.pdf', '/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/doc/rainw/rainw3.pdf', '/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/doc/rainw/rainw2.pdf', '/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/doc/gutter/gutter2.pdf', '/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/doc/gutter/gutter3.pdf', '/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/doc/gutter/gutter1.pdf', '/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/doc/greenr/greenr4.pdf', '/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/doc/greenr/greenr2.pdf', '/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/doc/greenr/greenr1.pdf', '/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/doc/stormw/stormw3.pdf', '/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/doc/stormw/stormw5.pdf', '/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/doc/stormw/stormw6.pdf', '/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/doc/stormw/stormw4.pdf', '/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/doc/stormw/stormw2.pdf', '/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/doc/stormw/stormw1.pdf', '/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/doc/siteg/siteg2.pdf', '/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/doc/waterp/waterp2.pdf', '/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/doc/waterp/waterp1.pdf', '/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/doc/weept/weept1.pdf', '/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/doc/weept/weept2.pdf', '/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/doc/sumpp/sumpp1.pdf', '/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/doc/sumpp/sumpp3.pdf', '/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/doc/sumpp/sumpp4.pdf', '/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/doc/sumpp/sumpp2.pdf']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGDtLrXjkP7b"
      },
      "source": [
        "Convert pdf object into text format to parse:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wj5VHV2gkP7c",
        "outputId": "227fb8c6-baae-4989-ec1b-456fda50d1f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "texts = []\n",
        "for file in pdf_objects:\n",
        "    with open(file,'rb') as pdf:\n",
        "        print('loading pdf...')\n",
        "        text = extract_text(pdf)\n",
        "        texts.append(text)\n",
        "        print('pdf appended..')\n",
        "\n",
        "print('pdf read complete...')\n",
        "#check object\n",
        "#texts"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading pdf...\n",
            "pdf appended..\n",
            "loading pdf...\n",
            "pdf appended..\n",
            "loading pdf...\n",
            "pdf appended..\n",
            "loading pdf...\n",
            "pdf appended..\n",
            "loading pdf...\n",
            "pdf appended..\n",
            "loading pdf...\n",
            "pdf appended..\n",
            "loading pdf...\n",
            "pdf appended..\n",
            "loading pdf...\n",
            "pdf appended..\n",
            "loading pdf...\n",
            "pdf appended..\n",
            "loading pdf...\n",
            "pdf appended..\n",
            "loading pdf...\n",
            "pdf appended..\n",
            "loading pdf...\n",
            "pdf appended..\n",
            "loading pdf...\n",
            "pdf appended..\n",
            "loading pdf...\n",
            "pdf appended..\n",
            "loading pdf...\n",
            "pdf appended..\n",
            "loading pdf...\n",
            "pdf appended..\n",
            "loading pdf...\n",
            "pdf appended..\n",
            "loading pdf...\n",
            "pdf appended..\n",
            "loading pdf...\n",
            "pdf appended..\n",
            "loading pdf...\n",
            "pdf appended..\n",
            "loading pdf...\n",
            "pdf appended..\n",
            "loading pdf...\n",
            "pdf appended..\n",
            "loading pdf...\n",
            "pdf appended..\n",
            "loading pdf...\n",
            "pdf appended..\n",
            "loading pdf...\n",
            "pdf appended..\n",
            "loading pdf...\n",
            "pdf appended..\n",
            "loading pdf...\n",
            "pdf appended..\n",
            "loading pdf...\n",
            "pdf appended..\n",
            "loading pdf...\n",
            "pdf appended..\n",
            "loading pdf...\n",
            "pdf appended..\n",
            "loading pdf...\n",
            "pdf appended..\n",
            "loading pdf...\n",
            "pdf appended..\n",
            "loading pdf...\n",
            "pdf appended..\n",
            "loading pdf...\n",
            "pdf appended..\n",
            "loading pdf...\n",
            "pdf appended..\n",
            "loading pdf...\n",
            "pdf appended..\n",
            "loading pdf...\n",
            "pdf appended..\n",
            "loading pdf...\n",
            "pdf appended..\n",
            "loading pdf...\n",
            "pdf appended..\n",
            "loading pdf...\n",
            "pdf appended..\n",
            "loading pdf...\n",
            "pdf appended..\n",
            "pdf read complete...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cwdwE88kP7f"
      },
      "source": [
        "### Sentence tokenization using SpaCy Sentencizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7DkPVp1kP7g"
      },
      "source": [
        "# alternative code for sentence tokenization using nltk\n",
        "# tokens = tokenize.sent_tokenize(text)\n",
        "\n",
        "#sentence tokenization using english vocab and SpaCy Sentencizer\n",
        "#declare doc object to pass into our nlp pipeline\n",
        "all_text = \" \".join(str(x) for x in texts)    \n",
        "doc = nlp(all_text)\n",
        "\n",
        "#check number of sentences detected by Sentencizer\n",
        "print(\"Number of lines :\", (len(list(doc.sents))))\n",
        "\n",
        "# create a list of tokenized sentences\n",
        "sents = [sent.text.strip() for sent in doc.sents]\n",
        "\n",
        "#lemmatization\n",
        "\n",
        "#check sents list object\n",
        "#sents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGMBZ3EUkP7k"
      },
      "source": [
        "## DECLARE NEW CUSTOM ENTITY\n",
        "Code below setups and defines new custom entity and prepares the training data for the NER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhDegGYYkP7k"
      },
      "source": [
        "## New entity creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmCWQdaqkP7l"
      },
      "source": [
        "### Ruled based matching using PhraseMatcher - edit to add additional rules to capture more strings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MW6fmsNekP7l"
      },
      "source": [
        "'''#Entity creation -- works okay...\n",
        "#Label setup, we provide the label 'SUSTECH' for sustainability and resilience technologies.\n",
        "LABEL = 'SUSTECH'\n",
        "\n",
        "Define matcher object using english vocab we defined earlier. Test using PhraseMatcher\n",
        "to define rules based on the exact patterns the strings will take the form of\n",
        "\n",
        "#Character patterns to add into our matcher object\n",
        "rule_matcher = PhraseMatcher(nlp.vocab)\n",
        "\n",
        "# create rule patterns - ADD more rule patterns here!\n",
        "rule_patterns = ['Green roof', \n",
        "            'green roof', \n",
        "            'green Roof',\n",
        "            'Green Roof',\n",
        "            'stormwater pond',\n",
        "            'Stormwater Pond',\n",
        "            'Stormwater pond',\n",
        "            'Stormwater ponds',\n",
        "            'stormwater ponds',\n",
        "            'Stormwater Ponds',\n",
        "            'stormwater Ponds',\n",
        "            'Sump Pump',\n",
        "            'sump Pump',\n",
        "            'sump pump',\n",
        "            'Sump Pumps',\n",
        "            'sump Pumps',\n",
        "            'sump pumps',\n",
        "            'Backup Generator',\n",
        "            'backup Generator',\n",
        "            'Backup generator',\n",
        "            'backup generator',\n",
        "            'Backup Generators',\n",
        "            'backup Generators',\n",
        "            'Backup generators',\n",
        "            'backup generators']\n",
        "\n",
        "for i in rule_patterns: \n",
        "    rule_matcher.add(LABEL, None, nlp(i))'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql56D7EXkP7o"
      },
      "source": [
        "Test for pattern detection defined using phraseMatcher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VItplmvekP7p"
      },
      "source": [
        "'''# A simple test to check our matcher object\n",
        "test_doc = nlp(\"I have a broken green roof in my Green Roof hood.\")\n",
        "print(test_doc)\n",
        "\n",
        "for idx, start, end in rule_matcher(test_doc):\n",
        "    print(test_doc[start:end],)'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BAW5kvKkP7r"
      },
      "source": [
        "### Token based matching using Matcher - edit to add additional token patterns to capture more strings based on defined token attributes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuunC-lZkP7s"
      },
      "source": [
        "### Using Matcher - Need to improve\n",
        "'''\n",
        "Define matcher object using english vocab we defined earlier. \n",
        "Test using Matcher to define rules based on the token attributes of the string.\n",
        "'''\n",
        "\n",
        "LABEL = 'SUSTECH'\n",
        "\n",
        "token_matcher = Matcher(nlp.vocab, validate=True)\n",
        "\n",
        "# create token patterns - ADD more token patterns here!\n",
        "'''\n",
        "token_patterns = [\n",
        "    [{\"LOWER\": \"green\", 'POS': {'IN' : ['ADJ','NOUN', 'PROPN']}}, {\"LOWER\": \"roof\", 'POS': {'IN' : ['NOUN','PROPN']}}],\n",
        "    [{\"LOWER\": \"stormwater\", 'POS': {'IN' : ['ADJ','NOUN', 'PROPN']}},{\"LOWER\": \"pond\", 'POS': {'IN' : ['ADJ','NOUN', 'PROPN']}}],\n",
        "    [{\"LOWER\": \"stormwater\", 'POS': {'IN' : ['ADJ','NOUN', 'PROPN']}},{\"LOWER\": \"ponds\", 'POS': {'IN' : ['ADJ','NOUN', 'PROPN']}}],\n",
        "    [{\"LOWER\": \"sump\", 'POS': {'IN' : ['ADJ','NOUN', 'PROPN']}},{\"LOWER\": \"pump\", 'POS': {'IN' : ['ADJ','NOUN', 'PROPN']}}],\n",
        "    [{\"LOWER\": \"sump\", 'POS': {'IN' : ['ADJ','NOUN', 'PROPN']}},{\"LOWER\": \"pumps\", 'POS': {'IN' : ['ADJ','NOUN', 'PROPN']}}],\n",
        "    [{\"LOWER\": \"backup\", 'POS': {'IN' : ['ADJ','NOUN', 'PROPN']}},{\"LOWER\": \"generator\", 'POS': {'IN' : ['ADJ','NOUN', 'PROPN']}}],\n",
        "]\n",
        "'''\n",
        "\n",
        "\n",
        "token_patterns = [\n",
        "    [{\"LOWER\": \"green\"}, {\"LOWER\": \"roof\"}],\n",
        "    [{\"LOWER\": \"sump\"}, {\"LOWER\": \"pump\"}],\n",
        "    [{\"LOWER\": \"backwater\"},{\"LOWER\": \"valve\"}],\n",
        "    [{\"LOWER\": \"backup\"},{\"LOWER\": \"power\"}],\n",
        "    [{\"LOWER\": \"roof\"},{\"LOWER\": \"drain\"}, {\"LOWER\": \"cover\"}],\n",
        "    [{\"LOWER\": \"fuel\"},{\"LOWER\": \"storage\"}, {\"LOWER\": \"tank\"}],\n",
        "    [{\"LOWER\": \"downspout\"}],\n",
        "    [{\"LOWER\": \"weeping\"},{\"LOWER\": \"tiles\"}],\n",
        "    [{\"LOWER\": \"site\"},{\"LOWER\": \"grading\"}],\n",
        "    [{\"LOWER\": \"flood\"},{\"LOWER\": \"barrier\"}],\n",
        "    [{\"LOWER\": \"below\"},{\"LOWER\": \"ground\"}, {\"LOWER\": \"floor\"}],\n",
        "    [{\"LOWER\": \"support\"},{\"LOWER\": \"ground\"}, {\"LOWER\": \"room\"}],\n",
        "    [{\"LOWER\": \"rainwater\"}, {\"LOWER\": \"harvest\", \"LEMMA\": \"harvest\", \"POS\": {\"IN\": [\"NOUN\",\"VERB\"]}}],\n",
        "    [{\"LOWER\": \"rainwater\"}, {\"LOWER\": \"harvesting\", \"POS\": {\"IN\": [\"NOUN\"]}}],\n",
        "    [{\"LOWER\": \"gutter\"}],\n",
        "    #plural patterns\n",
        "    [{\"LOWER\": \"green\"}, {\"LOWER\": \"roofs\"}],\n",
        "    [{\"LOWER\": \"sump\"}, {\"LOWER\": \"pumps\"}],\n",
        "    [{\"LOWER\": \"backwater\"},{\"LOWER\": \"valves\"}],\n",
        "    [{\"LOWER\": \"backup\"},{\"LOWER\": \"powers\"}],\n",
        "    [{\"LOWER\": \"roof\"},{\"LOWER\": \"drain\"}, {\"LOWER\": \"covers\"}],\n",
        "    [{\"LOWER\": \"fuel\"},{\"LOWER\": \"storage\"}, {\"LOWER\": \"tanks\"}],\n",
        "    [{\"LOWER\": \"downspouts\"}],\n",
        "    [{\"LOWER\": \"weeping\"},{\"LOWER\": \"tiles\"}],\n",
        "    [{\"LOWER\": \"site\"},{\"LOWER\": \"gradings\"}],\n",
        "    [{\"LOWER\": \"flood\"},{\"LOWER\": \"barriers\"}],\n",
        "    [{\"LOWER\": \"below\"},{\"LOWER\": \"ground\"}, {\"LOWER\": \"floors\"}],\n",
        "    [{\"LOWER\": \"support\"},{\"LOWER\": \"ground\"}, {\"LOWER\": \"rooms\"}],\n",
        "    [{\"LOWER\": \"rainwater\"}, {\"LOWER\": \"harvests\", \"LEMMA\": \"harvests\", \"POS\": {\"IN\": [\"NOUN\", \"VERB\"]}}],\n",
        "    [{\"LOWER\": \"rainwater\"}, {\"LOWER\": \"harvestings\", \"LEMMA\": \"harvestings\", \"POS\": {\"IN\": [\"NOUN\"]}}],\n",
        "    [{\"LOWER\": \"gutters\"}]\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "token_matcher.add(LABEL, None, *token_patterns)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDA4WGJXkP7u"
      },
      "source": [
        "nlp.pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cG4dQHwGkP7x"
      },
      "source": [
        "text = (\n",
        "    '''I have a broken green roof in my Green Roof hood. There is no Stormwater Pond on this roof. I like stormwater ponds. \\n\n",
        "    The backup generator located on the 8th floor is not working. The Sump Pump is so broken. \\n\n",
        "    The rainwater harvest is not working.\n",
        "    ''')\n",
        "test_doc = nlp(text)\n",
        "words_lemmas_list = [token.lemma_ for token in test_doc]\n",
        "\n",
        "print(test_doc)\n",
        "print(words_lemmas_list)\n",
        "#tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
        "\n",
        "for idx, start, end in token_matcher(test_doc):\n",
        "    print(test_doc[start:end],)\n",
        "    \n",
        "tokens = [token.text for token in test_doc]\n",
        "tags = [token.tag for token in test_doc]\n",
        "displacy.render(test_doc, style='dep', jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1V9DohskP7z"
      },
      "source": [
        "## Utility function\n",
        "Recall in order to train the NER model, we require to annotate tokenized text that takes the form: [(Sentence, {entities: [(start, end, label)]}, ...]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27UVdI9NkP70"
      },
      "source": [
        "#define our utility function\n",
        "def train_parser(doc):\n",
        "    position = [(doc[start:end].start_char, doc[start:end].end_char, LABEL) for \n",
        "                  idx, start, end in token_matcher(doc)]\n",
        "    return (doc.text,  {'entities': position})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZytoKVgkP73"
      },
      "source": [
        "Tests for our utility function on our test doc object:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5THSSbXkP73"
      },
      "source": [
        "train_parser(test_doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPsUlf6mkP76"
      },
      "source": [
        "## Applying our utility function to our doc object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdVwuvD5kP76"
      },
      "source": [
        "TRAIN_DATA = [train_parser(d) for d in nlp.pipe(sents) if len(token_matcher(d))==1]\n",
        "TRAIN_DATA[0:10]\n",
        "#len(TRAIN_DATA)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ojCUSmHkP79"
      },
      "source": [
        "## Storing annotations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RapAlsAGkP79"
      },
      "source": [
        "train_path = r'./train/'\n",
        "TRAIN_DATA_OUTPUT = pd.DataFrame(TRAIN_DATA, columns=['text',\n",
        "                                                      'position'])\n",
        "TRAIN_DATA_OUTPUT.to_csv(os.path.join(train_path,'train.csv'), index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKTSyGwtkP8B"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}