{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font size=\"10\">Custom entity recognition </font>\n",
    "## Training data environment setup\n",
    "\n",
    "## Introduction:\n",
    "\n",
    "This notebook is test code to setup new entities and programmtically creating training data to train the NER model. The goal of the project is to tag environmental/sustainability technologies contained with in the systems data stored with in our capital planning systems.\n",
    "\n",
    "To skip to creation of training data and go directly to [model training and model implementation](./ner-model-note.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP\n",
    "Module downloads (necessary for only first instance): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_md==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.3.1/en_core_web_md-2.3.1.tar.gz#egg=en_core_web_md==2.3.1 in c:\\users\\csunj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in c:\\users\\csunj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from en_core_web_md==2.3.1) (2.3.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\csunj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (4.47.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\csunj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\csunj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\csunj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (3.0.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\csunj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (0.8.0)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\csunj\\AppData\\Local\\Programs\\Python\\Python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: setuptools in c:\\users\\csunj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (47.1.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\csunj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (2.24.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\csunj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.19.0)\n",
      "Requirement already satisfied: thinc==7.4.1 in c:\\users\\csunj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (7.4.1)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in c:\\users\\csunj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (0.4.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\csunj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\csunj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\csunj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (2.0.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\csunj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\csunj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\csunj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\csunj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.25.9)\n",
      "[+] Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "#!pip install spacy\n",
    "#!pip install pdfminer.six\n",
    "# !python -m spacy download en_core_web_md\n",
    "#general imports \n",
    "#!pip install nltk \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from io import StringIO\n",
    "\n",
    "#pdf miner \n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "#SpaCy \n",
    "import en_core_web_md\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.pipeline import Sentencizer\n",
    "from spacy.lemmatizer import Lemmatizer, ADJ, NOUN, VERB\n",
    "from spacy.lookups import Lookups\n",
    "#nltk\n",
    "from nltk import tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing data setup\n",
    "The code section below setup the testing for the NER."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relative path setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filepath is : ./test/systems.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "root = r'./test/'\n",
    "file = 'systems.csv'\n",
    "print('Filepath is :',(os.path.join( root, file)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaration and store testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append comments together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = [_ for _ in data['System - Comments']]\n",
    "description = [_ for _ in data['System - Description']]\n",
    "\n",
    "#combine list of text data\n",
    "for i in description:\n",
    "  comments.append(i)\n",
    "  \n",
    "#clean test\n",
    "comments = [x for x in comments if str(x) != 'nan']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train data setup\n",
    "The code section below setup the training data for the NER.\n",
    "The is defined in the following steps:\n",
    "* Convert documents into text\n",
    "* Setting up the SpaCy nlp object\n",
    "* Load, parsing and tokenization of text into a list of sentences\n",
    "* Utility function order to parse the tokenized sentences in order to inherit the form:  [(Sentence, {entities: [(start, end, label)]}, ...]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy object delcaration and relative path setup:\n",
    "\n",
    "SpaCy requires training data to be in the format: [(Sentence, {entities: [(start, end, label)]}, ...].\n",
    "We first need to create a list of sentences throughout a doc object containing the new entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.pipes.Tagger at 0x263b7425460>),\n",
       " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x263c93c5460>),\n",
       " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x263c93c5ee0>)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Declare english vocab for our nlp object to define english language sentence boundaries\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp = English()\n",
    "nlp = en_core_web_md.load()\n",
    "nlp.max_length = 6130000 # or higher\n",
    "\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare rules for sentence boundary detection logic and add to the nlp pipe object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x263cc3a9cd0>),\n",
       " ('tagger', <spacy.pipeline.pipes.Tagger at 0x263b7425460>),\n",
       " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x263c93c5460>),\n",
       " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x263c93c5ee0>)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentencizer = Sentencizer(punct_chars=[\".\", \"?\", \"!\", \"ã€‚\"])\n",
    "nlp.add_pipe(sentencizer, before=\"tagger\")\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup import doc import, write tests for single doc object for now:\n",
    "\n",
    "## Current test case is for the word 'green roof'\n",
    "\n",
    "Document path setup (can change to a higher level directory to capture more/different docs for other terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./doc\\\\backg\\\\backg1.pdf', './doc\\\\backg\\\\backg2.pdf', './doc\\\\backp\\\\backp1.pdf', './doc\\\\backw\\\\backw1.pdf', './doc\\\\backw\\\\backw2.pdf', './doc\\\\backw\\\\backw3.pdf', './doc\\\\downs\\\\downs1.pdf', './doc\\\\downs\\\\downs2.pdf', './doc\\\\downs\\\\downs3.pdf', './doc\\\\downs\\\\downs4.pdf', './doc\\\\floodb\\\\floodb1.pdf', './doc\\\\floodb\\\\floodb2.pdf', './doc\\\\fuels\\\\fuels1.pdf', './doc\\\\fuels\\\\fuels2.pdf', './doc\\\\fuels\\\\fuels3.pdf', './doc\\\\greenr\\\\greenr1.pdf', './doc\\\\greenr\\\\greenr2.pdf', './doc\\\\greenr\\\\greenr3.PDF', './doc\\\\greenr\\\\greenr4.pdf', './doc\\\\gutter\\\\gutter1.pdf', './doc\\\\gutter\\\\gutter2.pdf', './doc\\\\gutter\\\\gutter3.pdf', './doc\\\\rainw\\\\rainw1.pdf', './doc\\\\rainw\\\\rainw2.pdf', './doc\\\\rainw\\\\rainw3.pdf', './doc\\\\roofd\\\\roofd1.pdf', './doc\\\\roofd\\\\roofd2.pdf', './doc\\\\siteg\\\\siteg2.pdf', './doc\\\\stormw\\\\stormw1.pdf', './doc\\\\stormw\\\\stormw2.pdf', './doc\\\\stormw\\\\stormw3.pdf', './doc\\\\stormw\\\\stormw4.pdf', './doc\\\\stormw\\\\stormw5.pdf', './doc\\\\stormw\\\\stormw6.pdf', './doc\\\\sumpp\\\\sumpp1.pdf', './doc\\\\sumpp\\\\sumpp2.pdf', './doc\\\\sumpp\\\\sumpp3.pdf', './doc\\\\sumpp\\\\sumpp4.pdf', './doc\\\\waterp\\\\waterp1.pdf', './doc\\\\waterp\\\\waterp2.pdf', './doc\\\\weept\\\\weept1.pdf', './doc\\\\weept\\\\weept2.pdf']\n"
     ]
    }
   ],
   "source": [
    "'''doc_path = Path(r'./doc/stormw') #change this for broader directory to capture\n",
    "pdf_objects = list(doc_path.glob('stormw4.pdf'))  # convert result to a list\n",
    "print(pdf_objects)'''\n",
    "\n",
    "#Setup higher-level directory function to import more docs\n",
    "dir_path = r'./doc/' #change this for broader directory to capture\n",
    "pdf_objects = glob.glob(dir_path + '/**/*.pdf', recursive=True)\n",
    "print(pdf_objects)\n",
    "#test on single pdf object\n",
    "#pdf_object =  os.path.join(doc_path, 'example2.pdf')'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert pdf object into text format to parse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pdf...\n",
      "pdf appended..\n",
      "loading pdf...\n",
      "pdf appended..\n",
      "loading pdf...\n",
      "pdf appended..\n",
      "loading pdf...\n",
      "pdf appended..\n",
      "loading pdf...\n",
      "pdf appended..\n",
      "loading pdf...\n",
      "pdf appended..\n",
      "loading pdf...\n",
      "pdf appended..\n",
      "loading pdf...\n",
      "pdf appended..\n",
      "loading pdf...\n",
      "pdf appended..\n",
      "loading pdf...\n",
      "pdf appended..\n",
      "loading pdf...\n",
      "pdf appended..\n",
      "loading pdf...\n",
      "pdf appended..\n",
      "loading pdf...\n",
      "pdf appended..\n",
      "loading pdf...\n",
      "pdf appended..\n",
      "loading pdf...\n",
      "pdf appended..\n",
      "loading pdf...\n",
      "pdf appended..\n",
      "loading pdf...\n",
      "pdf appended..\n",
      "loading pdf...\n",
      "pdf appended..\n",
      "loading pdf...\n",
      "pdf appended..\n",
      "loading pdf...\n",
      "pdf appended..\n",
      "loading pdf...\n",
      "pdf appended..\n",
      "loading pdf...\n",
      "pdf appended..\n",
      "loading pdf...\n",
      "pdf appended..\n",
      "loading pdf...\n",
      "pdf appended..\n",
      "loading pdf...\n",
      "pdf appended..\n",
      "loading pdf...\n",
      "pdf appended..\n",
      "loading pdf...\n",
      "pdf appended..\n",
      "loading pdf...\n",
      "pdf appended..\n",
      "loading pdf...\n",
      "pdf appended..\n",
      "loading pdf...\n",
      "pdf appended..\n",
      "loading pdf...\n",
      "pdf appended..\n",
      "loading pdf...\n",
      "pdf appended..\n",
      "loading pdf...\n",
      "pdf appended..\n",
      "loading pdf...\n",
      "pdf appended..\n",
      "loading pdf...\n",
      "pdf appended..\n",
      "loading pdf...\n",
      "pdf appended..\n",
      "loading pdf...\n",
      "pdf appended..\n",
      "loading pdf...\n",
      "pdf appended..\n",
      "loading pdf...\n",
      "pdf appended..\n",
      "loading pdf...\n",
      "pdf appended..\n",
      "loading pdf...\n",
      "pdf appended..\n",
      "loading pdf...\n",
      "pdf appended..\n",
      "pdf read complete...\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "for file in pdf_objects:\n",
    "    with open(file,'rb') as pdf:\n",
    "        print('loading pdf...')\n",
    "        text = extract_text(pdf)\n",
    "        texts.append(text)\n",
    "        print('pdf appended..')\n",
    "\n",
    "print('pdf read complete...')\n",
    "#check object\n",
    "#texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence tokenization using SpaCy Sentencizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 333. MiB for an array with shape (908373, 96) and data type int32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-665dd2ba0893>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#declare doc object to pass into our nlp pipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mall_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#check number of sentences detected by Sentencizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\csunj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m    447\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"__call__\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.__call__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.predict\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.greedy_parse\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\users\\csunj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\thinc\\neural\\_classes\\model.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \"\"\"\n\u001b[1;32m--> 167\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\csunj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\thinc\\neural\\_classes\\model.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m_parser_model.pyx\u001b[0m in \u001b[0;36mspacy.syntax._parser_model.ParserModel.begin_update\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_parser_model.pyx\u001b[0m in \u001b[0;36mspacy.syntax._parser_model.ParserStepModel.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\users\\csunj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\thinc\\neural\\_classes\\feed_forward.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X, drop)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\csunj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\thinc\\api.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(seqs_in, drop)\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseqs_in\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mseqs_in\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbp_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseqs_in\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbp_layer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\csunj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\thinc\\neural\\_classes\\feed_forward.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X, drop)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\csunj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\thinc\\neural\\_classes\\resnet.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X, drop)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbp_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\csunj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\thinc\\neural\\_classes\\feed_forward.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X, drop)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\csunj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\thinc\\neural\\_classes\\layernorm.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X, drop)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchild\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackprop_child\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchild\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[0mbackprop_child\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\csunj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\thinc\\neural\\_classes\\maxout.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X__bi, drop)\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0moutput__boc\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnO\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnP\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0moutput__boc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput__boc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput__boc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnO\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m         \u001b[0mbest__bo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhich__bo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaxout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput__boc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m         \u001b[0mbest__bo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbp_dropout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest__bo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mops.pyx\u001b[0m in \u001b[0;36mthinc.neural.ops.NumpyOps.maxout\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 333. MiB for an array with shape (908373, 96) and data type int32"
     ]
    }
   ],
   "source": [
    "# alternative code for sentence tokenization using nltk\n",
    "# tokens = tokenize.sent_tokenize(text)\n",
    "\n",
    "#sentence tokenization using english vocab and SpaCy Sentencizer\n",
    "#declare doc object to pass into our nlp pipeline\n",
    "all_text = \" \".join(str(x) for x in texts)    \n",
    "doc = nlp(all_text)\n",
    "\n",
    "#check number of sentences detected by Sentencizer\n",
    "print(\"Number of lines :\", (len(list(doc.sents))))\n",
    "\n",
    "# create a list of tokenized sentences\n",
    "sents = [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "#lemmatization\n",
    "\n",
    "#check sents list object\n",
    "#sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DECLARE NEW CUSTOM ENTITY\n",
    "Code below setups and defines new custom entity and prepares the training data for the NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New entity creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ruled based matching using PhraseMatcher - edit to add additional rules to capture more strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#Entity creation -- works okay...\n",
    "#Label setup, we provide the label 'SUSTECH' for sustainability and resilience technologies.\n",
    "LABEL = 'SUSTECH'\n",
    "\n",
    "Define matcher object using english vocab we defined earlier. Test using PhraseMatcher\n",
    "to define rules based on the exact patterns the strings will take the form of\n",
    "\n",
    "#Character patterns to add into our matcher object\n",
    "rule_matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# create rule patterns - ADD more rule patterns here!\n",
    "rule_patterns = ['Green roof', \n",
    "            'green roof', \n",
    "            'green Roof',\n",
    "            'Green Roof',\n",
    "            'stormwater pond',\n",
    "            'Stormwater Pond',\n",
    "            'Stormwater pond',\n",
    "            'Stormwater ponds',\n",
    "            'stormwater ponds',\n",
    "            'Stormwater Ponds',\n",
    "            'stormwater Ponds',\n",
    "            'Sump Pump',\n",
    "            'sump Pump',\n",
    "            'sump pump',\n",
    "            'Sump Pumps',\n",
    "            'sump Pumps',\n",
    "            'sump pumps',\n",
    "            'Backup Generator',\n",
    "            'backup Generator',\n",
    "            'Backup generator',\n",
    "            'backup generator',\n",
    "            'Backup Generators',\n",
    "            'backup Generators',\n",
    "            'Backup generators',\n",
    "            'backup generators']\n",
    "\n",
    "for i in rule_patterns: \n",
    "    rule_matcher.add(LABEL, None, nlp(i))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for pattern detection defined using phraseMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# A simple test to check our matcher object\n",
    "test_doc = nlp(\"I have a broken green roof in my Green Roof hood.\")\n",
    "print(test_doc)\n",
    "\n",
    "for idx, start, end in rule_matcher(test_doc):\n",
    "    print(test_doc[start:end],)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token based matching using Matcher - edit to add additional token patterns to capture more strings based on defined token attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using Matcher - Need to improve\n",
    "'''\n",
    "Define matcher object using english vocab we defined earlier. \n",
    "Test using Matcher to define rules based on the token attributes of the string.\n",
    "'''\n",
    "\n",
    "LABEL = 'SUSTECH'\n",
    "\n",
    "token_matcher = Matcher(nlp.vocab, validate=True)\n",
    "\n",
    "# create token patterns - ADD more token patterns here!\n",
    "'''\n",
    "token_patterns = [\n",
    "    [{\"LOWER\": \"green\", 'POS': {'IN' : ['ADJ','NOUN', 'PROPN']}}, {\"LOWER\": \"roof\", 'POS': {'IN' : ['NOUN','PROPN']}}],\n",
    "    [{\"LOWER\": \"stormwater\", 'POS': {'IN' : ['ADJ','NOUN', 'PROPN']}},{\"LOWER\": \"pond\", 'POS': {'IN' : ['ADJ','NOUN', 'PROPN']}}],\n",
    "    [{\"LOWER\": \"stormwater\", 'POS': {'IN' : ['ADJ','NOUN', 'PROPN']}},{\"LOWER\": \"ponds\", 'POS': {'IN' : ['ADJ','NOUN', 'PROPN']}}],\n",
    "    [{\"LOWER\": \"sump\", 'POS': {'IN' : ['ADJ','NOUN', 'PROPN']}},{\"LOWER\": \"pump\", 'POS': {'IN' : ['ADJ','NOUN', 'PROPN']}}],\n",
    "    [{\"LOWER\": \"sump\", 'POS': {'IN' : ['ADJ','NOUN', 'PROPN']}},{\"LOWER\": \"pumps\", 'POS': {'IN' : ['ADJ','NOUN', 'PROPN']}}],\n",
    "    [{\"LOWER\": \"backup\", 'POS': {'IN' : ['ADJ','NOUN', 'PROPN']}},{\"LOWER\": \"generator\", 'POS': {'IN' : ['ADJ','NOUN', 'PROPN']}}],\n",
    "]\n",
    "'''\n",
    "\n",
    "\n",
    "token_patterns = [\n",
    "    [{\"LOWER\": \"green\"}, {\"LOWER\": \"roof\"}],\n",
    "    [{\"LOWER\": \"sump\"}, {\"LOWER\": \"pump\"}],\n",
    "    [{\"LOWER\": \"backwater\"},{\"LOWER\": \"valve\"}],\n",
    "    [{\"LOWER\": \"backup\"},{\"LOWER\": \"power\"}],\n",
    "    [{\"LOWER\": \"roof\"},{\"LOWER\": \"drain\"}, {\"LOWER\": \"cover\"}],\n",
    "    [{\"LOWER\": \"fuel\"},{\"LOWER\": \"storage\"}, {\"LOWER\": \"tank\"}],\n",
    "    [{\"LOWER\": \"downspout\"}],\n",
    "    [{\"LOWER\": \"weeping\"},{\"LOWER\": \"tiles\"}],\n",
    "    [{\"LOWER\": \"site\"},{\"LOWER\": \"grading\"}],\n",
    "    [{\"LOWER\": \"flood\"},{\"LOWER\": \"barrier\"}],\n",
    "    [{\"LOWER\": \"below\"},{\"LOWER\": \"ground\"}, {\"LOWER\": \"floor\"}],\n",
    "    [{\"LOWER\": \"support\"},{\"LOWER\": \"ground\"}, {\"LOWER\": \"room\"}],\n",
    "    [{\"LOWER\": \"rainwater\"}, {\"LOWER\": \"harvest\", \"LEMMA\": \"harvest\", \"POS\": {\"IN\": [\"NOUN\",\"VERB\"]}}],\n",
    "    [{\"LOWER\": \"rainwater\"}, {\"LOWER\": \"harvesting\", \"POS\": {\"IN\": [\"NOUN\"]}}],\n",
    "    [{\"LOWER\": \"gutter\"}],\n",
    "    #plural patterns\n",
    "    [{\"LOWER\": \"green\"}, {\"LOWER\": \"roofs\"}],\n",
    "    [{\"LOWER\": \"sump\"}, {\"LOWER\": \"pumps\"}],\n",
    "    [{\"LOWER\": \"backwater\"},{\"LOWER\": \"valves\"}],\n",
    "    [{\"LOWER\": \"backup\"},{\"LOWER\": \"powers\"}],\n",
    "    [{\"LOWER\": \"roof\"},{\"LOWER\": \"drain\"}, {\"LOWER\": \"covers\"}],\n",
    "    [{\"LOWER\": \"fuel\"},{\"LOWER\": \"storage\"}, {\"LOWER\": \"tanks\"}],\n",
    "    [{\"LOWER\": \"downspouts\"}],\n",
    "    [{\"LOWER\": \"weeping\"},{\"LOWER\": \"tiles\"}],\n",
    "    [{\"LOWER\": \"site\"},{\"LOWER\": \"gradings\"}],\n",
    "    [{\"LOWER\": \"flood\"},{\"LOWER\": \"barriers\"}],\n",
    "    [{\"LOWER\": \"below\"},{\"LOWER\": \"ground\"}, {\"LOWER\": \"floors\"}],\n",
    "    [{\"LOWER\": \"support\"},{\"LOWER\": \"ground\"}, {\"LOWER\": \"rooms\"}],\n",
    "    [{\"LOWER\": \"rainwater\"}, {\"LOWER\": \"harvests\", \"LEMMA\": \"harvests\", \"POS\": {\"IN\": [\"NOUN\", \"VERB\"]}}],\n",
    "    [{\"LOWER\": \"rainwater\"}, {\"LOWER\": \"harvestings\", \"LEMMA\": \"harvestings\", \"POS\": {\"IN\": [\"NOUN\"]}}],\n",
    "    [{\"LOWER\": \"gutters\"}]\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "token_matcher.add(LABEL, None, *token_patterns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\n",
    "    '''I have a broken green roof in my Green Roof hood. There is no Stormwater Pond on this roof. I like stormwater ponds. \\n\n",
    "    The backup generator located on the 8th floor is not working. The Sump Pump is so broken. \\n\n",
    "    The rainwater harvest is not working.\n",
    "    ''')\n",
    "test_doc = nlp(text)\n",
    "words_lemmas_list = [token.lemma_ for token in test_doc]\n",
    "\n",
    "print(test_doc)\n",
    "print(words_lemmas_list)\n",
    "#tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
    "\n",
    "for idx, start, end in token_matcher(test_doc):\n",
    "    print(test_doc[start:end],)\n",
    "    \n",
    "tokens = [token.text for token in test_doc]\n",
    "tags = [token.tag for token in test_doc]\n",
    "displacy.render(test_doc, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility function\n",
    "Recall in order to train the NER model, we require to annotate tokenized text that takes the form: [(Sentence, {entities: [(start, end, label)]}, ...]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define our utility function\n",
    "def train_parser(doc):\n",
    "    position = [(doc[start:end].start_char, doc[start:end].end_char, LABEL) for \n",
    "                  idx, start, end in token_matcher(doc)]\n",
    "    return (doc.text,  {'entities': position})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests for our utility function on our test doc object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_parser(test_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying our utility function to our doc object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = [train_parser(d) for d in nlp.pipe(sents) if len(token_matcher(d))==1]\n",
    "TRAIN_DATA[0:10]\n",
    "#len(TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = r'./train/'\n",
    "TRAIN_DATA_OUTPUT = pd.DataFrame(TRAIN_DATA, columns=['text',\n",
    "                                                      'position'])\n",
    "TRAIN_DATA_OUTPUT.to_csv(os.path.join(train_path,'train.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
