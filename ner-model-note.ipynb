{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.4"
    },
    "colab": {
      "name": "ner-model-note.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltTvGHBDFZ9G"
      },
      "source": [
        "# <font size=\"10\">Custom entity recognition </font>\n",
        "## Model environment setup\n",
        "\n",
        "This notebook contains test code to train the implemented model in the generated training data outputted from\n",
        "the [ner-train notebook located here](./ner-train-note.ipynb).\n",
        "\n",
        "For simple loop model training (old), go [here](## Run training - using simple training loop from blank -- Old)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVojE79MFZ9H"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jhBZ6QpwCq_",
        "outputId": "c18b562f-81bd-43a1-800f-7b00f9a748b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install -U spacy\n",
        "!pip install spacy[lookups]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spacy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/b5/c7a92c7ce5d4b353b70b4b5b4385687206c8b230ddfe08746ab0fd310a3a/spacy-2.3.2-cp36-cp36m-manylinux1_x86_64.whl (9.9MB)\n",
            "\u001b[K     |████████████████████████████████| 10.0MB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Collecting thinc==7.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/ae/ef3ae5e93639c0ef8e3eb32e3c18341e511b3c515fcfc603f4b808087651/thinc-7.4.1-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 48.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (50.3.2)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (2.0.0)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.3.1)\n",
            "Installing collected packages: thinc, spacy\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iIRtAeGPMVt"
      },
      "source": [
        "!python -m spacy download en\n",
        "\n",
        "!python -m spacy download en_core_web_md\n",
        "!python -m spacy validate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgTBkJsdFZ9H"
      },
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import warnings\n",
        "import json\n",
        "import ast\n",
        "import datetime as dt\n",
        "from pathlib import Path\n",
        "import os\n",
        "import glob\n",
        "from __future__ import unicode_literals, print_function\n",
        "\n",
        "##SpaCy\n",
        "\n",
        "import en_core_web_sm\n",
        "import en_core_web_md\n",
        "\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from spacy.lang.en import English\n",
        "from spacy.matcher import Matcher\n",
        "from spacy.matcher import PhraseMatcher\n",
        "from spacy.pipeline import Sentencizer\n",
        "from spacy.lemmatizer import Lemmatizer, ADJ, NOUN, VERB\n",
        "from spacy.util import minibatch, compounding\n",
        "from spacy.util import decaying\n",
        "from spacy.pipeline import Tagger\n",
        "from spacy.pipeline import DependencyParser\n",
        "from thinc.neural.optimizers import Adam\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4RRpdqkH_lT"
      },
      "source": [
        "# For Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrmzl2o-H9zW"
      },
      "source": [
        "#mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvvdrUCJOBmw"
      },
      "source": [
        "# For Local"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n57b_Y0ROEP-"
      },
      "source": [
        "'''train_path = r'./train/'\n",
        "\n",
        "csvs = glob.glob(train_path + \"/*.csv\")\n",
        "\n",
        "\n",
        "print('Filepath is :',(csvs))\n",
        "\n",
        "train_list = []\n",
        "\n",
        "for filename in csvs:\n",
        "    df = pd.read_csv(filename, index_col=None, header=0)\n",
        "    train_list.append(df)'''\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_t-LuqCFZ9M"
      },
      "source": [
        "## Import training data\n",
        "\n",
        "lets import the training data we generated:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXFr8AiEFZ9M"
      },
      "source": [
        "train_path = r'/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/train/'\n",
        "\n",
        "csvs = glob.glob(train_path + \"/*.csv\")\n",
        "\n",
        "\n",
        "print('Filepath is :',(csvs))\n",
        "\n",
        "train_list = []\n",
        "\n",
        "for filename in csvs:\n",
        "    df = pd.read_csv(filename, index_col=None, header=0)\n",
        "    train_list.append(df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxUwACdFFZ9Q"
      },
      "source": [
        "DATA = pd.concat(train_list, axis=0, ignore_index=True)\n",
        "DATA = DATA.drop_duplicates()\n",
        "DATA[0:10]\n",
        "print(len(DATA))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9cYvMPJFZ9S"
      },
      "source": [
        "#convert to list for model intake\n",
        "TRAIN_DATA = DATA.values.tolist()\n",
        "\n",
        "#for element in index 1 convert string (Entity position) to dictionary to be able to read by the model function\n",
        "for position in TRAIN_DATA:\n",
        "    position[1]=ast.literal_eval(position[1])\n",
        "    \n",
        "#Check our input list\n",
        "print(TRAIN_DATA[0:2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbQYerDEFZ9V"
      },
      "source": [
        "## Run a test before training\n",
        "### Test existing default spacy model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSJ4_q9SFZ9V"
      },
      "source": [
        "nlp = en_core_web_md.load()\n",
        "nlp.pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pG06VFQ9FZ9b"
      },
      "source": [
        "doc = nlp('Here is a green roof on this house. A green roof is good.')\n",
        "displacy.render(doc, style=\"ent\")\n",
        "# verified green roof does not match an entity in the NER"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kD6c9-LPFZ9f"
      },
      "source": [
        "## TRAINING THE MODEL\n",
        "## Train model setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmFkte5ZFZ9f"
      },
      "source": [
        "### Define compounding batch size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aDY1Rp5FZ9g"
      },
      "source": [
        "def get_batches(train_data, model_type):\n",
        "    max_batch_sizes = {\"tagger\": 32, \"parser\": 16, \"ner\": 16, \"textcat\": 64}\n",
        "    max_batch_size = max_batch_sizes[model_type]\n",
        "    if len(train_data) < 1000:\n",
        "        max_batch_size /= 2\n",
        "    if len(train_data) < 500:\n",
        "        max_batch_size /= 2\n",
        "    batch_size = compounding(1, max_batch_size, 1.001)\n",
        "    batches = minibatch(train_data, size=batch_size)\n",
        "    return batches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bl2TKaXdK9a7"
      },
      "source": [
        "## Define custom Adam optimizer\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYsRD2lqK8iM"
      },
      "source": [
        "\n",
        "def custom_optimizer(optimizer, learn_rate=0.001, beta1=0.9, beta2=0.999, eps=1e-8, L2=1e-6, max_grad_norm=1.0):\n",
        "    \"\"\"\n",
        "    Function to customizer spaCy default optimizer\n",
        "    \"\"\"\n",
        "    \n",
        "    optimizer.learn_rate = learn_rate\n",
        "    optimizer.beta1 = beta1\n",
        "    optimizer.beta2 = beta2\n",
        "    optimizer.eps = eps\n",
        "    optimizer.L2 = L2\n",
        "    optimizer.max_grad_norm = max_grad_norm\n",
        "    \n",
        "    return optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcjN2-28LCqr"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boH2r7RFFZ9j"
      },
      "source": [
        "\n",
        "def train_model(**model_params):\n",
        "\n",
        "    model = model_params['model']\n",
        "    iterations = model_params['iterations']\n",
        "    train_data = model_params['train_data']\n",
        "    dropout = model_params['dropout'],\n",
        "    learn_rate = model_params['learn_rate'], \n",
        "    beta1 = model_params['beta1'], \n",
        "    beta2 = model_params['beta2'], \n",
        "    eps = model_params['eps'], \n",
        "    L2 = model_params['L2'], \n",
        "    max_grad_norm = model_params['max_grad_norm']\n",
        "\n",
        "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
        "    \n",
        "    if model is not None:\n",
        "        nlp = spacy.load(model)  # load existing spaCy model\n",
        "        print(\"Loaded model '%s'\" % model)\n",
        "    else:\n",
        "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
        "        print(\"Created blank 'en' model\")\n",
        "\n",
        "    # create the built-in pipeline components and add them to the pipeline\n",
        "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
        "    if \"ner\" not in nlp.pipe_names:\n",
        "        ner = nlp.create_pipe(\"ner\")\n",
        "        nlp.add_pipe(ner, last=True)\n",
        "    # otherwise, get it so we can add labels\n",
        "    else:\n",
        "        nlp.remove_pipe(\"ner\")\n",
        "        ner = nlp.create_pipe(\"ner\")\n",
        "        nlp.add_pipe(ner, last=True)\n",
        "        ner = nlp.get_pipe(\"ner\")\n",
        "\n",
        "    # add labels\n",
        "    for _, annotations in TRAIN_DATA:\n",
        "        for ent in annotations.get(\"entities\"):\n",
        "            ner.add_label(ent[2])\n",
        "\n",
        "    # get names of other pipes to disable them during training\n",
        "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
        "    \n",
        "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "        # reset and initialize the weights randomly – but only if we're\n",
        "        # training a new model\n",
        "        \n",
        "        if model is None:\n",
        "            optimizer = nlp.begin_training(component_cfg={\"ner\": {\"conv_window\": 3}})\n",
        "            optimizer=custom_optimizer(optimizer, learn_rate=learn_rate)\n",
        "        \n",
        "        else:\n",
        "            optimizer = nlp.resume_training(component_cfg={\"ner\": {\"conv_window\": 3}})\n",
        "            optimizer=custom_optimizer(optimizer, learn_rate=learn_rate)\n",
        "\n",
        "\n",
        "        # Define decaying dropout\n",
        "        dropout = decaying(0.6, 0.2, 1e-4)\n",
        "        \n",
        "        for itn in range(n_iter):\n",
        "            random.shuffle(TRAIN_DATA)\n",
        "            losses = {}\n",
        "            # batch up the examples using spaCy's minibatch\n",
        "            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
        "            for batch in batches:\n",
        "                texts, annotations = zip(*batch)\n",
        "                nlp.update(\n",
        "                    texts,  # batch of texts\n",
        "                    annotations,  # batch of annotations\n",
        "                    drop=next(dropout),  # dropout - make it harder to memorise data\n",
        "                    sgd= optimizer,\n",
        "                    losses=losses,\n",
        "                )\n",
        "        print(f\"Losses at iteration {itn} - {dt.datetime.now()} {losses}\")\n",
        "    \n",
        "    print('Model training completed')\n",
        "    return nlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hZNCZF5FZ9l"
      },
      "source": [
        "model_params = {\n",
        "    'model': en_core_web_md,\n",
        "    'iterations': 40,\n",
        "    'train_data': TRAIN_DATA,\n",
        "    'dropout': decaying(0.6, 0.2, 1e-4),\n",
        "    'learn_rate':0.001, \n",
        "    'beta1': 0.9, \n",
        "    'beta2': 0.999, \n",
        "    'eps': 1e-8, \n",
        "    'L2': 1e-6, \n",
        "    'max_grad_norm':1.0\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUqwsny-FZ9n"
      },
      "source": [
        "nlp = train_model(**model_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RenoBfkNFZ9p"
      },
      "source": [
        "## Test the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOfzjej7FZ9q"
      },
      "source": [
        "nlp.pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oxbkbyzflek"
      },
      "source": [
        "\n",
        "#tagger = Tagger(nlp.vocab)\n",
        "#parser = DependencyParser(nlp.vocab)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sTGG9CliL7Z"
      },
      "source": [
        "nlp.pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-yQg_LmiQkD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YelALnlkFZ9u"
      },
      "source": [
        "doc = nlp('Here is a green roof on this house. A green roof is good. water piping, I have alot of battery packs')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tiR_oCMFZ9y"
      },
      "source": [
        "displacy.render(doc, style=\"ent\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gMfzK8kFZ90"
      },
      "source": [
        "## Save model for testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZTl9AXjFZ91"
      },
      "source": [
        "output_dir = r'/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/model'\n",
        "\n",
        "if output_dir is not None:\n",
        "    output_dir = Path(output_dir)\n",
        "    if not output_dir.exists():\n",
        "        output_dir.mkdir()\n",
        "    nlp.to_disk(output_dir)\n",
        "    print(\"Saved model to\", output_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsHUZX0YOWDz"
      },
      "source": [
        "'''output_dir = r'./model'\n",
        "\n",
        "if output_dir is not None:\n",
        "    output_dir = Path(output_dir)\n",
        "    if not output_dir.exists():\n",
        "        output_dir.mkdir()\n",
        "    nlp.to_disk(output_dir)\n",
        "    print(\"Saved model to\", output_dir)'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "almVk7DNFZ93"
      },
      "source": [
        "## Loading and testing the saved model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcaNNa6BFZ93"
      },
      "source": [
        "x = ['i am a green roof']\n",
        "print(\"Loading from\", output_dir)\n",
        "nlp2 = spacy.load(output_dir)\n",
        "\n",
        "for text in x:\n",
        "    doc = nlp2(text)\n",
        "    print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
        "    print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rZ_vlXPOpgE"
      },
      "source": [
        "## For Local"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIlynPFUiJyK"
      },
      "source": [
        "'''output_dir = r'./model'\n",
        "\n",
        "if output_dir is not None:\n",
        "    output_dir = Path(output_dir)\n",
        "    if not output_dir.exists():\n",
        "        output_dir.mkdir()\n",
        "    nlp.to_disk(output_dir)\n",
        "    print(\"Saved model to\", output_dir)'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GagHlUrVOt0l"
      },
      "source": [
        "'''output_dir = r'./model'\n",
        "x = ['rainwater harvesting is great']\n",
        "print(\"Loading from\", output_dir)\n",
        "nlp2 = spacy.load(output_dir)\n",
        "\n",
        "for text in x:\n",
        "    doc = nlp2(text)\n",
        "    print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
        "    print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])'''"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}