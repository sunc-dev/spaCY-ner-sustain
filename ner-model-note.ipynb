{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.4"
    },
    "colab": {
      "name": "ner-model-note.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltTvGHBDFZ9G"
      },
      "source": [
        "# <font size=\"10\">Custom entity recognition </font>\n",
        "## Model environment setup\n",
        "\n",
        "This notebook contains test code to train the implemented model in the generated training data outputted from\n",
        "the [ner-train notebook located here](./ner-train-note.ipynb).\n",
        "\n",
        "For simple loop model training (old), go [here](## Run training - using simple training loop from blank -- Old)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVojE79MFZ9H"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jhBZ6QpwCq_",
        "outputId": "a82373b7-a961-4087-ac97-bb40ac473aca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install -U spacy\n",
        "!pip install spacy[lookups]\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spacy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/b5/c7a92c7ce5d4b353b70b4b5b4385687206c8b230ddfe08746ab0fd310a3a/spacy-2.3.2-cp36-cp36m-manylinux1_x86_64.whl (9.9MB)\n",
            "\u001b[K     |████████████████████████████████| 10.0MB 2.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (50.3.2)\n",
            "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n",
            "Collecting thinc==7.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/ae/ef3ae5e93639c0ef8e3eb32e3c18341e511b3c515fcfc603f4b808087651/thinc-7.4.1-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 31.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (2.0.0)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.3.1)\n",
            "Installing collected packages: thinc, spacy\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed spacy-2.3.2 thinc-7.4.1\n",
            "Requirement already satisfied: spacy[lookups] in /usr/local/lib/python3.6/dist-packages (2.3.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy[lookups]) (1.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy[lookups]) (50.3.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy[lookups]) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy[lookups]) (1.18.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy[lookups]) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy[lookups]) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy[lookups]) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy[lookups]) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy[lookups]) (2.0.3)\n",
            "Requirement already satisfied: thinc==7.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy[lookups]) (7.4.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy[lookups]) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy[lookups]) (0.8.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy[lookups]) (3.0.2)\n",
            "Collecting spacy-lookups-data<0.4.0,>=0.3.2; extra == \"lookups\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/b7/d5c635e51718c874606fb08249c0fab710286240d54847dc60bad3dfceac/spacy_lookups_data-0.3.2.tar.gz (93.8MB)\n",
            "\u001b[K     |████████████████████████████████| 93.8MB 51kB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy[lookups]) (2.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy[lookups]) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy[lookups]) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy[lookups]) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy[lookups]) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy[lookups]) (3.3.1)\n",
            "Building wheels for collected packages: spacy-lookups-data\n",
            "  Building wheel for spacy-lookups-data (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spacy-lookups-data: filename=spacy_lookups_data-0.3.2-py2.py3-none-any.whl size=93807573 sha256=11a0560a9eca6844337757c6785bbbec6743c773d7897b9e6c9c5e8d74b69812\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/f4/d0/bf720a06127c95d9be2a81d197a3f1998ee5fc63410944e28f\n",
            "Successfully built spacy-lookups-data\n",
            "Installing collected packages: spacy-lookups-data\n",
            "Successfully installed spacy-lookups-data-0.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iIRtAeGPMVt",
        "outputId": "b65d8136-0fb8-4f14-e78a-f604426e12ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python -m spacy download en\n",
        "\n",
        "!python -m spacy download en_core_web_md\n",
        "!python -m spacy validate"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_sm==2.3.1\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0MB)\n",
            "\u001b[K     |████████████████████████████████| 12.1MB 1.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.3.1) (2.3.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.41.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (50.3.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.18.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.6.20)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.3.1)\n",
            "Building wheels for collected packages: en-core-web-sm\n",
            "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.3.1-cp36-none-any.whl size=12047109 sha256=1bc02f4cf86d6ac1cb17ff3630f38c02cbf02addae3338ee2d0f46819ab90d8e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-i2f85mho/wheels/2b/3f/41/f0b92863355c3ba34bb32b37d8a0c662959da0058202094f46\n",
            "Successfully built en-core-web-sm\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "Successfully installed en-core-web-sm-2.3.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Collecting en_core_web_md==2.3.1\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.3.1/en_core_web_md-2.3.1.tar.gz (50.8MB)\n",
            "\u001b[K     |████████████████████████████████| 50.8MB 58.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from en_core_web_md==2.3.1) (2.3.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.0.2)\n",
            "Requirement already satisfied: thinc==7.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (7.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (4.41.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (3.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (50.3.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (2.0.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (0.8.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.18.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (2.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (3.3.1)\n",
            "Building wheels for collected packages: en-core-web-md\n",
            "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-md: filename=en_core_web_md-2.3.1-cp36-none-any.whl size=50916643 sha256=00b4f718431fb9af881de8969e1baa0ac0783606541a5b6b80615bf383ec92ae\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jkeiyfub/wheels/6e/65/3a/34cdc26d4084d1d1f1e2ec9914964759ea17aa382c53a57d9f\n",
            "Successfully built en-core-web-md\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-2.3.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n",
            "\u001b[2K\u001b[38;5;2m✔ Loaded compatibility table\u001b[0m\n",
            "\u001b[1m\n",
            "====================== Installed models (spaCy v2.3.2) ======================\u001b[0m\n",
            "\u001b[38;5;4mℹ spaCy installation: /usr/local/lib/python3.6/dist-packages/spacy\u001b[0m\n",
            "\n",
            "TYPE      NAME             MODEL            VERSION                            \n",
            "package   en-core-web-sm   en_core_web_sm   \u001b[38;5;2m2.3.1\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
            "package   en-core-web-md   en_core_web_md   \u001b[38;5;2m2.3.1\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
            "link      en               en_core_web_sm   \u001b[38;5;2m2.3.1\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgTBkJsdFZ9H"
      },
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import warnings\n",
        "import json\n",
        "import ast\n",
        "import datetime as dt\n",
        "from pathlib import Path\n",
        "import os\n",
        "import glob\n",
        "from __future__ import unicode_literals, print_function\n",
        "\n",
        "##SpaCy\n",
        "\n",
        "import en_core_web_sm\n",
        "import en_core_web_md\n",
        "\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from spacy.lang.en import English\n",
        "from spacy.matcher import Matcher\n",
        "from spacy.matcher import PhraseMatcher\n",
        "from spacy.pipeline import Sentencizer\n",
        "from spacy.lemmatizer import Lemmatizer, ADJ, NOUN, VERB\n",
        "from spacy.util import minibatch, compounding\n",
        "from spacy.util import decaying\n",
        "from spacy.pipeline import Tagger\n",
        "from spacy.pipeline import DependencyParser\n",
        "from thinc.neural.optimizers import Adam\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4RRpdqkH_lT"
      },
      "source": [
        "# For Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80O4ekMtUU0v"
      },
      "source": [
        ""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrmzl2o-H9zW",
        "outputId": "5d58145e-5357-414f-c9b0-1fb70bde572d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvvdrUCJOBmw"
      },
      "source": [
        "# For Local"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n57b_Y0ROEP-",
        "outputId": "b156fcfc-6903-4b91-e6c2-d71d0ab23c42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "'''train_path = r'./train/'\n",
        "\n",
        "csvs = glob.glob(train_path + \"/*.csv\")\n",
        "\n",
        "\n",
        "print('Filepath is :',(csvs))\n",
        "\n",
        "train_list = []\n",
        "\n",
        "for filename in csvs:\n",
        "    df = pd.read_csv(filename, index_col=None, header=0)\n",
        "    train_list.append(df)'''\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'train_path = r\\'./train/\\'\\n\\ncsvs = glob.glob(train_path + \"/*.csv\")\\n\\n\\nprint(\\'Filepath is :\\',(csvs))\\n\\ntrain_list = []\\n\\nfor filename in csvs:\\n    df = pd.read_csv(filename, index_col=None, header=0)\\n    train_list.append(df)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_t-LuqCFZ9M"
      },
      "source": [
        "## Import training data\n",
        "\n",
        "lets import the training data we generated:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXFr8AiEFZ9M",
        "outputId": "f40fb9e1-8fdb-437b-886e-3f3289ec415f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_path = r'/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/train/'\n",
        "\n",
        "csvs = glob.glob(train_path + \"/*.csv\")\n",
        "\n",
        "\n",
        "print('Filepath is :',(csvs))\n",
        "\n",
        "train_list = []\n",
        "\n",
        "for filename in csvs:\n",
        "    df = pd.read_csv(filename, index_col=None, header=0)\n",
        "    train_list.append(df)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filepath is : ['/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/train/trainset0.csv', '/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/train/trainset1.csv', '/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/train/trainset2.csv', '/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/train/trainset3.csv']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxUwACdFFZ9Q",
        "outputId": "5976a9f4-1f6e-4fb5-8d52-ec9621bb13a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "DATA = pd.concat(train_list, axis=0, ignore_index=True)\n",
        "DATA = DATA.drop_duplicates()\n",
        "DATA[0:10]\n",
        "print(len(DATA))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3345\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9cYvMPJFZ9S",
        "outputId": "3b69d10b-e831-4123-99c9-f2deb3f06575",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#convert to list for model intake\n",
        "TRAIN_DATA = DATA.values.tolist()\n",
        "\n",
        "#for element in index 1 convert string (Entity position) to dictionary to be able to read by the model function\n",
        "for position in TRAIN_DATA:\n",
        "    position[1]=ast.literal_eval(position[1])\n",
        "    \n",
        "#Check our input list\n",
        "TRAIN_DATA[0:10]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['With increasing severe weather events and disasters \\ntriggering greater numbers of costly power outages, there is a growing interest in generators for \\nreliable backup power.',\n",
              "  {'entities': [(161, 173, 'SUSTECH')]}],\n",
              " ['$3,153 \\n\\nNPC of Backup Power per Unit ($/kW)',\n",
              "  {'entities': [(16, 28, 'SUSTECH')]}],\n",
              " ['Annualized Costs and Benefits per Unit of Backup Power ($/kW-year).',\n",
              "  {'entities': [(42, 54, 'SUSTECH')]}],\n",
              " ['Many businesses without backup are \\nconsidering installing generators, and facilities such as hospitals and airports—which are \\nrequired to have backup power—are considering redundant systems for added resilience against \\ngrid outages.',\n",
              "  {'entities': [(145, 157, 'SUSTECH')]}],\n",
              " ['Revenues from additional services can \\nsignificantly reduce the life-cycle costs of a backup power system.',\n",
              "  {'entities': [(86, 98, 'SUSTECH')]}],\n",
              " ['Generators that provide \\ngrid services in addition to backup power are often run more frequently, which generally \\ncoincides with a lower FTS rate.',\n",
              "  {'entities': [(54, 66, 'SUSTECH')]}],\n",
              " ['analysis \\nof backup power systems at commercial and military facilities, which estimated an MTTF of 545 \\nhours for standby packaged diesel generators and an MTTF of 457 hours for all standby diesel \\ngenerators.8',\n",
              "  {'entities': [(13, 25, 'SUSTECH')]}],\n",
              " ['Where P is the availability of backup power, m is the likelihood of a generator being out of \\noperation due maintenance, FTS is the likelihood of it failing to start, FTR is the hourly \\nlikelihood of it failing to run, and FFS is the likelihood of a failure of fuel supply at or before',\n",
              "  {'entities': [(31, 43, 'SUSTECH')]}],\n",
              " ['The primary economic parameter to be determined is the life-cycle cost of backup power over \\nthe assumed life of the backup generators.',\n",
              "  {'entities': [(74, 86, 'SUSTECH')]}],\n",
              " ['In \\nanother business model, an energy services provider could install and own the generators and \\nprovide backup power to the facility owner for a fee.',\n",
              "  {'entities': [(106, 118, 'SUSTECH')]}]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbQYerDEFZ9V"
      },
      "source": [
        "## Run a test before training\n",
        "### Test existing default spacy model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSJ4_q9SFZ9V",
        "outputId": "5e2f8636-961a-4ae9-a88e-f1fbda3d4ce8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nlp = en_core_web_md.load()\n",
        "nlp.pipeline"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('tagger', <spacy.pipeline.pipes.Tagger at 0x7fc51476fef0>),\n",
              " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x7fc5162618e8>),\n",
              " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x7fc515635ee8>)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pG06VFQ9FZ9b",
        "outputId": "25d7e227-fd6c-48fc-b88d-ec913effa447",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "doc = nlp('Here is a green roof on this house. A green roof is good.')\n",
        "displacy.render(doc, style=\"ent\")\n",
        "# verified green roof does not match an entity in the NER"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/spacy/displacy/__init__.py:189: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n",
            "  warnings.warn(Warnings.W006)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Here is a green roof on this house. A green roof is good.</div>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kD6c9-LPFZ9f"
      },
      "source": [
        "## TRAINING THE MODEL\n",
        "## Train model setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmFkte5ZFZ9f"
      },
      "source": [
        "### Define compounding batch size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aDY1Rp5FZ9g"
      },
      "source": [
        "def get_batches(train_data, model_type):\n",
        "    max_batch_sizes = {\"tagger\": 32, \"parser\": 16, \"ner\": 16, \"textcat\": 64}\n",
        "    max_batch_size = max_batch_sizes[model_type]\n",
        "    if len(train_data) < 1000:\n",
        "        max_batch_size /= 2\n",
        "    if len(train_data) < 500:\n",
        "        max_batch_size /= 2\n",
        "    batch_size = compounding(1, max_batch_size, 1.001)\n",
        "    batches = minibatch(train_data, size=batch_size)\n",
        "    return batches"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bl2TKaXdK9a7"
      },
      "source": [
        "## Define custom Adam optimizer\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYsRD2lqK8iM"
      },
      "source": [
        "\n",
        "def custom_optimizer(optimizer, learn_rate=0.001, beta1=0.9, beta2=0.999, eps=1e-8, L2=1e-6, max_grad_norm=1.0):\n",
        "    \"\"\"\n",
        "    Function to customizer spaCy default optimizer\n",
        "    \"\"\"\n",
        "    \n",
        "    optimizer.learn_rate = learn_rate\n",
        "    optimizer.beta1 = beta1\n",
        "    optimizer.beta2 = beta2\n",
        "    optimizer.eps = eps\n",
        "    optimizer.L2 = L2\n",
        "    optimizer.max_grad_norm = max_grad_norm\n",
        "    \n",
        "    return optimizer"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcjN2-28LCqr"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boH2r7RFFZ9j"
      },
      "source": [
        "#New training model loop to either accept existing model, if not model is not defined then create a blank nlp model using english vocab\n",
        "\n",
        "def train_model(train_data = TRAIN_DATA, model=en_core_web_md, output_dir=None, n_iter=100,\n",
        "        learn_rate=0.001, \n",
        "        beta1=0.9, \n",
        "        beta2=0.999, \n",
        "        eps=1e-8, \n",
        "        L2=1e-6, \n",
        "        max_grad_norm=1.0):\n",
        "   \n",
        "   \n",
        "    random.seed(0)\n",
        "    \n",
        "    if model is not None:\n",
        "        nlp = model.load() #load existing spacy model\n",
        "        print(\"Loaded model '%s'\" % model)\n",
        "    else:\n",
        "        nlp = spacy.blank(\"en\")\n",
        "        #lang = \"en\"\n",
        "        #cls = spacy.util.get_lang_class(lang)   # 1. Get Language instance, e.g. English()\n",
        "        #nlp = cls() \n",
        "        print(\"Created blank 'en' model\")\n",
        "    \n",
        "    if \"ner\" not in nlp.pipe_names:\n",
        "        ner = nlp.create_pipe(\"ner\")\n",
        "        nlp.add_pipe(ner, last=True)\n",
        "    # otherwise,get it, so we can add labels to it\n",
        "    else:\n",
        "        nlp.remove_pipe(\"ner\")\n",
        "        ner = nlp.create_pipe(\"ner\")\n",
        "        nlp.add_pipe(ner, last=True)\n",
        "        ner = nlp.get_pipe(\"ner\")\n",
        "\n",
        "    #ner.add_label(LABEL)  # add new entity label to entity recognizer\n",
        "    for _, annotations in train_data:\n",
        "        for ent in annotations.get(\"entities\"):\n",
        "            ner.add_label(ent[2])\n",
        "            #print(ent[2])\n",
        "\n",
        "    # Adding extraneous labels shouldn't mess anything up\n",
        "    ner.add_label(\"VEGETABLE\")\n",
        "    move_names = list(ner.move_names)\n",
        "    # get names of other pipes to disable them during training\n",
        "    \n",
        "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
        "    \n",
        "    # only train NER\n",
        "    \n",
        "    with nlp.disable_pipes(*other_pipes), warnings.catch_warnings():\n",
        "        # show warnings for misaligned entity spans once\n",
        "        \n",
        "        warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\n",
        "        if model is None:\n",
        "            optimizer = nlp.begin_training(component_cfg={\"ner\": {\"conv_window\": 3}})\n",
        "            optimizer=custom_optimizer(optimizer, learn_rate=learn_rate)\n",
        "\n",
        "        else:\n",
        "            optimizer = nlp.begin_training(component_cfg={\"ner\": {\"conv_window\": 3}})\n",
        "            optimizer=custom_optimizer(optimizer, learn_rate=learn_rate)\n",
        "\n",
        "\n",
        "        # Define decaying dropout\n",
        "        dropout = decaying(0.6, 0.2, 1e-4)\n",
        "\n",
        "        # reset and initialize the weights randomly – but only if we're\n",
        "        # training a new model\n",
        "        for itn in range(n_iter):\n",
        "            random.shuffle(train_data)\n",
        "            #batches = get_batches(TRAIN_DATA, 'ner') resulted in poor loss\n",
        "            batches = minibatch(train_data,  size=compounding(4.0, 32.0, 1.001))\n",
        "            losses = {}\n",
        "            for batch in batches:\n",
        "                texts, annotations = zip(*batch)\n",
        "                nlp.update(texts, \n",
        "                           annotations,\n",
        "                           sgd=optimizer, \n",
        "                           drop = next(dropout), \n",
        "                           losses=losses)\n",
        "            print(f\"Losses at iteration {itn} - {dt.datetime.now()} {losses}\")\n",
        "    \n",
        "    print('Model training completed')\n",
        "    return nlp"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hZNCZF5FZ9l",
        "outputId": "63880528-751a-40a6-f150-8baa2264182a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "model_params = {\n",
        "    'model': en_core_web_md,\n",
        "    'iterations': 40,\n",
        "    'train_data': TRAIN_DATA,\n",
        "    'dropout': decaying(0.6, 0.2, 1e-4),\n",
        "    'learn_rate':0.001, \n",
        "    'beta1': 0.9, \n",
        "    'beta2': 0.999, \n",
        "    'eps': 1e-8, \n",
        "    'L2': 1e-6, \n",
        "    'max_grad_norm':1.0\n",
        "}\n",
        "\n",
        "#begin training customizable options \n",
        "'''\n",
        "\"beam_width\":1,\n",
        "\"beam_density\":0.0,\n",
        "\"beam_update_prob\":1.0,\n",
        "\"cnn_maxout_pieces\":3,\n",
        "\"nr_feature_tokens\":6,\n",
        "\"nr_class\":10,\n",
        "\"hidden_depth\":1,\n",
        "\"token_vector_width\":96,\n",
        "\"hidden_width\":64,\n",
        "\"maxout_pieces\":2,\n",
        "\"pretrained_vectors\":null,\n",
        "\"bilstm_depth\":0,\n",
        "\"self_attn_depth\":0,\n",
        "\"conv_depth\":4,\n",
        "\"conv_window\":3,\n",
        "\"embed_size\":2000\n",
        "'''"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\"beam_width\":1,\\n\"beam_density\":0.0,\\n\"beam_update_prob\":1.0,\\n\"cnn_maxout_pieces\":3,\\n\"nr_feature_tokens\":6,\\n\"nr_class\":10,\\n\"hidden_depth\":1,\\n\"token_vector_width\":96,\\n\"hidden_width\":64,\\n\"maxout_pieces\":2,\\n\"pretrained_vectors\":null,\\n\"bilstm_depth\":0,\\n\"self_attn_depth\":0,\\n\"conv_depth\":4,\\n\"conv_window\":3,\\n\"embed_size\":2000\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUqwsny-FZ9n",
        "outputId": "19c6e1c3-eca5-493a-dd19-368ff2d2f7fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nlp = train_model()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded model '<module 'en_core_web_md' from '/usr/local/lib/python3.6/dist-packages/en_core_web_md/__init__.py'>'\n",
            "Losses at iteration 0 - 2020-10-30 13:32:13.119738 {'ner': 3638.325384122061}\n",
            "Losses at iteration 1 - 2020-10-30 13:32:49.284914 {'ner': 335.5636046736778}\n",
            "Losses at iteration 2 - 2020-10-30 13:33:25.418781 {'ner': 156.6510573006446}\n",
            "Losses at iteration 3 - 2020-10-30 13:34:04.190750 {'ner': 117.71803143928365}\n",
            "Losses at iteration 4 - 2020-10-30 13:34:43.227142 {'ner': 110.36717738721175}\n",
            "Losses at iteration 5 - 2020-10-30 13:35:22.387430 {'ner': 81.36658645003399}\n",
            "Losses at iteration 6 - 2020-10-30 13:36:01.646504 {'ner': 63.29825762713743}\n",
            "Losses at iteration 7 - 2020-10-30 13:36:40.620315 {'ner': 73.74074334600974}\n",
            "Losses at iteration 8 - 2020-10-30 13:37:20.004232 {'ner': 58.04187835534385}\n",
            "Losses at iteration 9 - 2020-10-30 13:37:59.175295 {'ner': 48.62490626366566}\n",
            "Losses at iteration 10 - 2020-10-30 13:38:40.679754 {'ner': 40.91310435015577}\n",
            "Losses at iteration 11 - 2020-10-30 13:39:20.916301 {'ner': 53.46206652166811}\n",
            "Losses at iteration 12 - 2020-10-30 13:40:02.877793 {'ner': 28.615059943097155}\n",
            "Losses at iteration 13 - 2020-10-30 13:40:46.278257 {'ner': 26.68263072986753}\n",
            "Losses at iteration 14 - 2020-10-30 13:41:45.325893 {'ner': 32.88753382329388}\n",
            "Losses at iteration 15 - 2020-10-30 13:42:49.191125 {'ner': 32.38044571511976}\n",
            "Losses at iteration 16 - 2020-10-30 13:43:45.818793 {'ner': 56.12506802397595}\n",
            "Losses at iteration 17 - 2020-10-30 13:44:59.457367 {'ner': 34.92122394373636}\n",
            "Losses at iteration 18 - 2020-10-30 13:46:31.668773 {'ner': 21.63090277686178}\n",
            "Losses at iteration 19 - 2020-10-30 13:47:39.879628 {'ner': 34.78698488639138}\n",
            "Losses at iteration 20 - 2020-10-30 13:48:51.003814 {'ner': 44.41030563958305}\n",
            "Losses at iteration 21 - 2020-10-30 13:50:11.127351 {'ner': 38.8927224809319}\n",
            "Losses at iteration 22 - 2020-10-30 13:51:29.131847 {'ner': 51.834110621740145}\n",
            "Losses at iteration 23 - 2020-10-30 13:52:45.741809 {'ner': 29.07011490801716}\n",
            "Losses at iteration 24 - 2020-10-30 13:54:02.508638 {'ner': 18.4630653741093}\n",
            "Losses at iteration 25 - 2020-10-30 13:55:29.675992 {'ner': 25.48094124018322}\n",
            "Losses at iteration 26 - 2020-10-30 13:56:59.754699 {'ner': 22.652734892529057}\n",
            "Losses at iteration 27 - 2020-10-30 13:58:30.751794 {'ner': 12.27237985444667}\n",
            "Losses at iteration 28 - 2020-10-30 14:00:00.333184 {'ner': 17.926497648488727}\n",
            "Losses at iteration 29 - 2020-10-30 14:01:26.117938 {'ner': 11.805552334930109}\n",
            "Losses at iteration 30 - 2020-10-30 14:02:58.882267 {'ner': 21.20846274292022}\n",
            "Losses at iteration 31 - 2020-10-30 14:04:29.717959 {'ner': 27.224387435131213}\n",
            "Losses at iteration 32 - 2020-10-30 14:05:57.674579 {'ner': 24.873567442068538}\n",
            "Losses at iteration 33 - 2020-10-30 14:07:19.925710 {'ner': 13.178611429712754}\n",
            "Losses at iteration 34 - 2020-10-30 14:08:42.125699 {'ner': 13.964830347202804}\n",
            "Losses at iteration 35 - 2020-10-30 14:09:49.031683 {'ner': 4.17462078579418}\n",
            "Losses at iteration 36 - 2020-10-30 14:10:56.491388 {'ner': 18.035766754729153}\n",
            "Losses at iteration 37 - 2020-10-30 14:12:01.495949 {'ner': 20.199217710383728}\n",
            "Losses at iteration 38 - 2020-10-30 14:13:18.329061 {'ner': 16.392523496266364}\n",
            "Losses at iteration 39 - 2020-10-30 14:14:47.726788 {'ner': 3.1595213109924694}\n",
            "Losses at iteration 40 - 2020-10-30 14:16:20.083116 {'ner': 2.6942066312951296e-06}\n",
            "Losses at iteration 41 - 2020-10-30 14:17:39.181137 {'ner': 1.3811526046498248}\n",
            "Losses at iteration 42 - 2020-10-30 14:18:57.665085 {'ner': 17.059997828284438}\n",
            "Losses at iteration 43 - 2020-10-30 14:19:53.280309 {'ner': 33.182105499291865}\n",
            "Losses at iteration 44 - 2020-10-30 14:20:52.756215 {'ner': 28.49826826894113}\n",
            "Losses at iteration 45 - 2020-10-30 14:22:05.135322 {'ner': 52.15393309122612}\n",
            "Losses at iteration 46 - 2020-10-30 14:23:32.368083 {'ner': 13.774450163680479}\n",
            "Losses at iteration 47 - 2020-10-30 14:25:01.922318 {'ner': 18.597594342877997}\n",
            "Losses at iteration 48 - 2020-10-30 14:26:11.808278 {'ner': 9.620195122805757}\n",
            "Losses at iteration 49 - 2020-10-30 14:27:11.255593 {'ner': 3.3629587119570252}\n",
            "Losses at iteration 50 - 2020-10-30 14:28:14.810432 {'ner': 7.394189965096596}\n",
            "Losses at iteration 51 - 2020-10-30 14:29:27.793293 {'ner': 12.035347620994244}\n",
            "Losses at iteration 52 - 2020-10-30 14:30:30.518233 {'ner': 33.90025429957958}\n",
            "Losses at iteration 53 - 2020-10-30 14:32:01.290461 {'ner': 20.147214086179073}\n",
            "Losses at iteration 54 - 2020-10-30 14:33:22.641165 {'ner': 39.51537011867906}\n",
            "Losses at iteration 55 - 2020-10-30 14:34:56.935560 {'ner': 3.572127765616773}\n",
            "Losses at iteration 56 - 2020-10-30 14:36:17.208879 {'ner': 12.761646726728415}\n",
            "Losses at iteration 57 - 2020-10-30 14:37:47.726875 {'ner': 4.18103962153648}\n",
            "Losses at iteration 58 - 2020-10-30 14:38:57.459022 {'ner': 8.450963599564236}\n",
            "Losses at iteration 59 - 2020-10-30 14:40:02.232180 {'ner': 13.72781270163428}\n",
            "Losses at iteration 60 - 2020-10-30 14:41:06.875002 {'ner': 13.734303667565753}\n",
            "Losses at iteration 61 - 2020-10-30 14:42:16.332699 {'ner': 7.482022319443233}\n",
            "Losses at iteration 62 - 2020-10-30 14:43:20.967249 {'ner': 20.337082972702998}\n",
            "Losses at iteration 63 - 2020-10-30 14:44:12.238940 {'ner': 0.671563341158889}\n",
            "Losses at iteration 64 - 2020-10-30 14:45:03.978512 {'ner': 3.9974875987236356}\n",
            "Losses at iteration 65 - 2020-10-30 14:45:57.721138 {'ner': 38.8046703975504}\n",
            "Losses at iteration 66 - 2020-10-30 14:46:53.372061 {'ner': 9.857523011549988}\n",
            "Losses at iteration 67 - 2020-10-30 14:47:49.937198 {'ner': 7.143140473109229}\n",
            "Losses at iteration 68 - 2020-10-30 14:48:44.661909 {'ner': 13.556453163729019}\n",
            "Losses at iteration 69 - 2020-10-30 14:49:47.107005 {'ner': 7.075923505009621}\n",
            "Losses at iteration 70 - 2020-10-30 14:50:52.340425 {'ner': 1.7960729579862728e-05}\n",
            "Losses at iteration 71 - 2020-10-30 14:52:02.792280 {'ner': 21.5652466097167}\n",
            "Losses at iteration 72 - 2020-10-30 14:53:09.601428 {'ner': 6.145919305835616}\n",
            "Losses at iteration 73 - 2020-10-30 14:54:13.327942 {'ner': 0.08253190906982226}\n",
            "Losses at iteration 74 - 2020-10-30 14:55:21.404324 {'ner': 14.319226345178887}\n",
            "Losses at iteration 75 - 2020-10-30 14:56:21.726969 {'ner': 17.58743706405282}\n",
            "Losses at iteration 76 - 2020-10-30 14:57:23.548240 {'ner': 2.2995842108474314}\n",
            "Losses at iteration 77 - 2020-10-30 14:58:26.827967 {'ner': 10.004432336301683}\n",
            "Losses at iteration 78 - 2020-10-30 14:59:33.811077 {'ner': 3.9816553165420956}\n",
            "Losses at iteration 79 - 2020-10-30 15:00:42.782089 {'ner': 12.672686480149133}\n",
            "Losses at iteration 80 - 2020-10-30 15:01:41.059640 {'ner': 2.4235393279316026}\n",
            "Losses at iteration 81 - 2020-10-30 15:02:37.798105 {'ner': 11.834362418988366}\n",
            "Losses at iteration 82 - 2020-10-30 15:03:46.483000 {'ner': 17.11987937191414}\n",
            "Losses at iteration 83 - 2020-10-30 15:04:48.534472 {'ner': 9.659962038259806}\n",
            "Losses at iteration 84 - 2020-10-30 15:05:54.514267 {'ner': 25.013516970159635}\n",
            "Losses at iteration 85 - 2020-10-30 15:06:57.063445 {'ner': 16.20553142662062}\n",
            "Losses at iteration 86 - 2020-10-30 15:08:06.125926 {'ner': 1.668898227586324}\n",
            "Losses at iteration 87 - 2020-10-30 15:09:09.291520 {'ner': 2.1614420412226134}\n",
            "Losses at iteration 88 - 2020-10-30 15:10:11.282329 {'ner': 11.361600945518827}\n",
            "Losses at iteration 89 - 2020-10-30 15:11:15.001314 {'ner': 6.871215420158699}\n",
            "Losses at iteration 90 - 2020-10-30 15:12:23.264807 {'ner': 8.568432245222287}\n",
            "Losses at iteration 91 - 2020-10-30 15:13:24.408264 {'ner': 22.243950439939113}\n",
            "Losses at iteration 92 - 2020-10-30 15:14:25.251190 {'ner': 4.498770652685998}\n",
            "Losses at iteration 93 - 2020-10-30 15:15:31.035740 {'ner': 2.011675354367464}\n",
            "Losses at iteration 94 - 2020-10-30 15:16:33.851558 {'ner': 4.00468885945294}\n",
            "Losses at iteration 95 - 2020-10-30 15:17:37.715616 {'ner': 10.88092135238991}\n",
            "Losses at iteration 96 - 2020-10-30 15:18:44.034373 {'ner': 9.83304491339722}\n",
            "Losses at iteration 97 - 2020-10-30 15:19:52.931762 {'ner': 9.386043440522373}\n",
            "Losses at iteration 98 - 2020-10-30 15:20:56.186290 {'ner': 27.932617188764617}\n",
            "Losses at iteration 99 - 2020-10-30 15:21:58.041023 {'ner': 0.0008326812429808541}\n",
            "Model training completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RenoBfkNFZ9p"
      },
      "source": [
        "## Test the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOfzjej7FZ9q",
        "outputId": "4d26ca59-fffa-4af7-9a90-f7442e7737ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nlp.pipeline "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('tagger', <spacy.pipeline.pipes.Tagger at 0x7fc5108e8128>),\n",
              " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x7fc5116e5948>),\n",
              " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x7fc510a8cfa8>)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCizsJij-y9I"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YelALnlkFZ9u"
      },
      "source": [
        "doc = nlp('Here is a green roof on this house. A green roof is good. water piping, I have alot of battery packs')\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tiR_oCMFZ9y",
        "outputId": "1da3cf9a-1647-464d-f55d-dbba1777bbd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "displacy.render(doc, style=\"ent\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Here is a \\n<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\\n    green roof\\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">SUSTECH</span>\\n</mark>\\n on this house. A green roof is good. water piping, I have alot of battery packs</div>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gMfzK8kFZ90"
      },
      "source": [
        "## Save model for testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZTl9AXjFZ91",
        "outputId": "f503dcae-abc8-46a7-8754-c9835888729f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "output_dir = r'/content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/model'\n",
        "\n",
        "if output_dir is not None:\n",
        "    output_dir = Path(output_dir)\n",
        "    if not output_dir.exists():\n",
        "        output_dir.mkdir()\n",
        "    nlp.to_disk(output_dir)\n",
        "    print(\"Saved model to\", output_dir)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model to /content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsHUZX0YOWDz",
        "outputId": "3c4fb219-8c33-46c7-c92a-7cecfa362334",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "'''output_dir = r'./model'\n",
        "\n",
        "if output_dir is not None:\n",
        "    output_dir = Path(output_dir)\n",
        "    if not output_dir.exists():\n",
        "        output_dir.mkdir()\n",
        "    nlp.to_disk(output_dir)\n",
        "    print(\"Saved model to\", output_dir)'''"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'output_dir = r\\'./model\\'\\n\\nif output_dir is not None:\\n    output_dir = Path(output_dir)\\n    if not output_dir.exists():\\n        output_dir.mkdir()\\n    nlp.to_disk(output_dir)\\n    print(\"Saved model to\", output_dir)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "almVk7DNFZ93"
      },
      "source": [
        "## Loading and testing the saved model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcaNNa6BFZ93",
        "outputId": "f7552e7c-c6d5-42ce-b227-a8838cba8147",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x = ['i am a green roof']\n",
        "print(\"Loading from\", output_dir)\n",
        "nlp2 = spacy.load(output_dir)\n",
        "\n",
        "for text in x:\n",
        "    doc = nlp2(text)\n",
        "    print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
        "    print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading from /content/drive/My Drive/Colab Notebooks/nlp-ner-sustain-notebook/model\n",
            "Entities [('green roof', 'SUSTECH')]\n",
            "Tokens [('i', '', 2), ('am', '', 2), ('a', '', 2), ('green', 'SUSTECH', 3), ('roof', 'SUSTECH', 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rZ_vlXPOpgE"
      },
      "source": [
        "## For Local"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIlynPFUiJyK",
        "outputId": "d9f6f020-5f20-4f53-b345-c36910651237",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "'''output_dir = r'./model'\n",
        "\n",
        "if output_dir is not None:\n",
        "    output_dir = Path(output_dir)\n",
        "    if not output_dir.exists():\n",
        "        output_dir.mkdir()\n",
        "    nlp.to_disk(output_dir)\n",
        "    print(\"Saved model to\", output_dir)'''"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'output_dir = r\\'./model\\'\\n\\nif output_dir is not None:\\n    output_dir = Path(output_dir)\\n    if not output_dir.exists():\\n        output_dir.mkdir()\\n    nlp.to_disk(output_dir)\\n    print(\"Saved model to\", output_dir)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GagHlUrVOt0l",
        "outputId": "4d4f4b31-d8dc-4949-eed2-6e4b248324c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "'''output_dir = r'./model'\n",
        "x = ['rainwater harvesting is great']\n",
        "print(\"Loading from\", output_dir)\n",
        "nlp2 = spacy.load(output_dir)\n",
        "\n",
        "for text in x:\n",
        "    doc = nlp2(text)\n",
        "    print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
        "    print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])'''"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'output_dir = r\\'./model\\'\\nx = [\\'rainwater harvesting is great\\']\\nprint(\"Loading from\", output_dir)\\nnlp2 = spacy.load(output_dir)\\n\\nfor text in x:\\n    doc = nlp2(text)\\n    print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\\n    print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    }
  ]
}